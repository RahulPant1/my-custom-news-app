# Multi-Provider LLM Configuration
# Providers are tried in the exact order listed below
# First available provider with non-rate-limited models is used

# Default rate limits (can be overridden per model)
defaults:
  rpm: 3        # requests per minute (reduced to prevent hopping)
  rpd: 100      # requests per day (increased for better daily limits)
  timeout: 45    # timeout in seconds (increased for better reliability)

# Provider sequence (tried in this exact order)
providers:

  - name: ollama
    # Local models via Ollama - fastest, no API costs, no rate limits
    host: "http://localhost:11434"  # Default Ollama host
    models:
      - model: llama3.2:3b-instruct-q8_0
        rpm: 50      # High rate limit for local models
        rpd: 2000     # Very high daily limit
        timeout: 60    # Longer timeout for local inference

  - name: groq
    # Fast inference with high rate limits - good for development/testing
    models:
      - model: llama-3.1-8b-instant 
        rpm: 10        # Groq typically allows higher rates
        rpd: 200     # Very generous daily limits
        timeout: 30
      - model: llama-3.3-70b-versatile 
        rpm: 10
        rpd: 100
        timeout: 30
      - model: deepseek-r1-distill-llama-70b
        # Uses default rpm/rpd/timeout from above

  - name: google
    models:
      - model: gemini-2.5-pro
        rpm: 5       # max 5 requests/minute
        rpd: 50     # max 500 requests/day
        timeout: 30
      - model: gemini-2.0-flash
        rpm: 6
        rpd: 200
        timeout: 30
      - model: gemini-1.5-flash
        rpm: 5
        rpd: 15
        timeout: 30

  # - name: anthropic
  #   models:
  #     - model: claude-3-5-sonnet-20241022
  #       rpm: 5
  #       rpd: 300
  #       timeout: 60
  #     - model: claude-3-haiku-20240307
  #       rpm: 10
  #       rpd: 1000
  #       timeout: 30

  # - name: openai
  #   models:
  #     - model: gpt-4o
  #       rpm: 5
  #       rpd: 200
  #       timeout: 60
  #     - model: gpt-4o-mini
  #       rpm: 10
  #       rpd: 1000
  #       timeout: 30
  #     - model: gpt-3.5-turbo
  #       rpm: 15
  #       rpd: 2000
  #       timeout: 30

  - name: openrouter
    # Single API for multiple models - fallback option
    models:
      - model: anthropic/claude-3.5-sonnet
        rpm: 10
        rpd: 100
        timeout: 60
      - model: meta-llama/llama-3.1-8b-instruct:free
        rpm: 10        # Free tier usually has higher limits
        rpd: 200
        timeout: 30
      - model: openai/gpt-3.5-turbo
        # Uses default values - good for cost-effective queries

# Rate limiting settings
rate_limiting:
  enabled: true
  storage_backend: json  # Options: json, sqlite
  storage_path: usage_counters.json
  cleanup_interval: 3600  # Clean old data every hour

# Logging settings
logging:
  enabled: true
  log_requests: true
  log_rate_limits: true
  log_errors: true
  log_provider_selection: true